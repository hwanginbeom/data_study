{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon(펀다 4등 코드).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7HgJQnGczaT"
      },
      "source": [
        "# jupyter notebook cell 너비 조절\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z07tAlctcVuw"
      },
      "source": [
        "# 기본\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.externals import joblib \n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# 시계열\n",
        "from fbprophet import Prophet\n",
        "from datetime import datetime as dt\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.tsa.api import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
        "\n",
        "# 회귀분석\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "\n",
        "# Deep Neural Network\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "\n",
        "# 설정\n",
        "%matplotlib inline\n",
        "pd.options.display.max_columns = 400\n",
        "pd.options.display.float_format = '{:.5f}'.format\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUkyrJ2ldGlV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mroHdbP1c09e"
      },
      "source": [
        "def mae(prediction, correct):\n",
        "    prediction = np.array(prediction)\n",
        "    correct = np.array(correct)\n",
        "    \n",
        "    difference = correct - prediction\n",
        "    abs_val = abs(difference)\n",
        "    \n",
        "    score = abs_val.mean()\n",
        "    \n",
        "    return score\n",
        "\n",
        "mae_scorer = make_scorer(mae)\n",
        "mae_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWKkDTYdZA2"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/data/dacon/funda_train.csv')\n",
        "df_sub = pd.read_csv('/content/drive/MyDrive/data/dacon/submission.csv')\n",
        "df_train['transacted_date'] = pd.to_datetime(df_train['transacted_date'])\n",
        "\n",
        "print(df_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9fpxttbd3DT"
      },
      "source": [
        "frame_info = [(col, len(df_train[col].unique()), df_train[col].isnull().sum(), df_train[col].dtype, df_train[col].unique()[:5]) for col in df_train.columns]\n",
        "df_info = pd.DataFrame(frame_info, columns=['name', 'num_of_unique', 'num_of_nan', 'type', 'front5_values'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbIpUUG5d5GW"
      },
      "source": [
        "df_infodf_train = df_train.set_index('transacted_date')\n",
        "df_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aghs7nAxd8Sg"
      },
      "source": [
        "df_train = df_train.set_index('transacted_date')\n",
        "df_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBGd0dHUeK_f"
      },
      "source": [
        "def resample_day(train_df):\n",
        "    df_day = pd.DataFrame()\n",
        "    for i in train_df.store_id.unique():\n",
        "        df_num = train_df[train_df.store_id == i]\n",
        "\n",
        "        count_cols = df_num['card_id'].resample(rule='d').count().rename('num_of_pay') # 'card_id' 의 일별 counting을 통해 일 거래 횟수 확인\n",
        "        # 'card_id' value count가 2보다 크면 단골인 것으로 판단하고 단골 방문 횟수 확인\n",
        "        revisit_idx = df_num.card_id.value_counts().reset_index().query(\"card_id > 2\")[\"index\"].values \n",
        "        revisit_ct = df_num[df_num.card_id.isin(revisit_idx)].card_id.resample(rule='d').count().rename('num_of_revisit')  \n",
        "        sum_cols = df_num[['installment_term', 'amount']].resample(rule='d').sum() # 할부 개월수와 매출액은 일 단위로 합\n",
        "\n",
        "        df_num_day = pd.concat([count_cols, revisit_ct, sum_cols], axis=1)\n",
        "\n",
        "        df_num_day.insert(0, 'store_id', i)\n",
        "        df_num_day.insert(4, 'region', df_num[df_num.store_id == i].region.unique()[0])\n",
        "        df_num_day.insert(5, 'type_of_business', df_num[df_num.store_id == i].type_of_business.unique()[0])\n",
        "\n",
        "        df_day = pd.concat([df_day, df_num_day], axis=0)\n",
        "        \n",
        "    df_day.insert(1, 'day_of_week', df_day.index.dayofweek)\n",
        "    df_day.insert(2, 'business_day', df_day.day_of_week.replace({0:1, 2:1, 3:1, 4:1, 5:0, 6:0}).values)\n",
        "    df_day.num_of_revisit.fillna(0, inplace=True)\n",
        "    \n",
        "    return df_day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN3Zi_9ggjrZ"
      },
      "source": [
        "%%time\n",
        "df_day = resample_day(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKrP8pE2glYv"
      },
      "source": [
        "df_day.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13iOw6iihbVu"
      },
      "source": [
        "df_day.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7JhSi8ohgjC"
      },
      "source": [
        "df_day.to_csv('/content/drive/My Drive/data/dacon/funda_train_day.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qhaJJmihpnD"
      },
      "source": [
        "df_day = pd.read_csv('/content/drive/My Drive/data/dacon/funda_train_day.csv')\n",
        "df_day['transacted_date'] = pd.to_datetime(df_day['transacted_date'])\n",
        "df_day = df_day.set_index('transacted_date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6VV1QQhzAQ"
      },
      "source": [
        "daily_corr = df_day.corr()\n",
        "daily_corr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16rNBIEWh0hC"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(10, 7)\n",
        "sns.heatmap(daily_corr, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tDgIWRvh37A"
      },
      "source": [
        "def resample_month(frame_day):\n",
        "    sum_cols = ['num_of_pay', 'num_of_revisit', 'installment_term', 'amount']\n",
        "\n",
        "    df_monthly = pd.DataFrame()\n",
        "\n",
        "    for i in frame_day.store_id.unique():\n",
        "        df_set = frame_day[frame_day.store_id == i]\n",
        "        \n",
        "        # nan값이 발생하는 경우를 없애기 위해 이전, 이후 달에 대한 정보를 추가한 후 제거\n",
        "        prev_date = pd.date_range(start=(df_set.index[0] - relativedelta(months=1)), end=(df_set.index[0] - relativedelta(months=1)))\n",
        "        add_date = pd.date_range(start=(df_set.index[-1] + relativedelta(months=1)), end=(df_set.index[-1] + relativedelta(months=1)))\n",
        "        df_set = pd.concat([pd.DataFrame(index=prev_date), df_set, pd.DataFrame(index=add_date)], axis=0)\n",
        "\n",
        "        df_set.loc[dt.strftime(df_set.index[0], '%Y-%m'), :] = 1\n",
        "        df_set.loc[dt.strftime(df_set.index[-1], '%Y-%m'), :] = 1\n",
        "\n",
        "        tot_day = df_set[df_set.amount != 0].day_of_week.resample(rule='m').count().rename('real_tot_day')\n",
        "        business = df_set[df_set.amount != 0].business_day.resample(rule='m').sum().rename('real_business_day')\n",
        "\n",
        "        business = business.drop([business.index[0], business.index[-1]], axis=0)\n",
        "        tot_day = tot_day.drop([tot_day.index[0], tot_day.index[-1]], axis=0)\n",
        "        df_set = df_set.drop([df_set.index[0], df_set.index[-1]], axis=0)\n",
        "\n",
        "        df = pd.concat([tot_day, business, df_set[sum_cols].resample(rule='m').sum()], axis=1)\n",
        "\n",
        "        df.insert(0, 'store_id', i)\n",
        "        df.insert(6, 'region', df_set.region.values[0])\n",
        "        df.insert(7, 'type_of_business', df_set.type_of_business.values[0])\n",
        "\n",
        "        df_monthly = pd.concat([df_monthly, df], axis=0)\n",
        "   \n",
        "    return df_monthly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hohnq9GTidQn"
      },
      "source": [
        "%%time\n",
        "df_month = resample_month(df_day)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws6ngTE7ieZF"
      },
      "source": [
        "df_month.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9u-zrW-iiAT"
      },
      "source": [
        "print(\"rate of 'region' :\" , df_month.region.isnull().sum() / len(df_month))\n",
        "print(\"rate of 'type_of_business' :\" , df_month.type_of_business.isnull().sum() / len(df_month))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTBE3apSiqD5"
      },
      "source": [
        "df_month = df_month.drop(['region', 'type_of_business'], axis=1)\n",
        "df_month.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PPws7TJjFgk"
      },
      "source": [
        "df_month.to_csv('/content/drive/My Drive/data/dacon/funda_train_month.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjfD0GemiwrP"
      },
      "source": [
        "df_month = pd.read_csv('/content/drive/My Drive/data/dacon/funda_train_month.csv')\n",
        "df_month.rename(columns={'Unnamed: 0' : 'transacted_date'}, inplace=True)\n",
        "df_month['transacted_date'] = pd.to_datetime(df_month['transacted_date'])\n",
        "df_month = df_month.set_index('transacted_date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qgPo92vjIus"
      },
      "source": [
        "print(df_month.shape)\n",
        "df_month.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYN_-eNqjPbX"
      },
      "source": [
        "df_month.describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oItzxsfJjRq6"
      },
      "source": [
        "# target 변수의 대칭성(정규성) 확인¶\n",
        "\n",
        "for i in df_month.store_id.unique()[:3]:\n",
        "    print(\"Skewness :\", df_month[df_month.store_id == i].amount.skew())\n",
        "    sns.distplot(df_month[df_month.store_id == i].amount)\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYNifbm_jTar"
      },
      "source": [
        "print(np.log(df_month[df_month.store_id == 2].amount).skew())\n",
        "sns.distplot(np.log(df_month[df_month.store_id == 2].amount))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42lBoXA5jalS"
      },
      "source": [
        "# 각 변수의 value 분포 시각화 각 변수의 value 분포 시각화\n",
        "f = pd.melt(df_month, value_vars=df_month.columns[1:])\n",
        "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False)\n",
        "g = g.map(sns.distplot, \"value\")\n",
        "plt.show() \n",
        "f = pd.melt(df_month, value_vars=df_month.columns[1:])\n",
        "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False)\n",
        "g = g.map(sns.distplot, \"value\")\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3kOe-HJjgyt"
      },
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "for i in df_month.store_id.unique()[:7]:\n",
        "    plt.plot(df_month[df_month.store_id == i].index, df_month[df_month.store_id == i].amount, label='store_id {}'.format(i))\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4G2Wfo2jyMl"
      },
      "source": [
        "plt.figure(figsize=(15, 3))\n",
        "plt.plot(df_month[df_month.store_id == 1].index, df_month[df_month.store_id == 1].amount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrByccDJkWvN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtEgzQmbkXVk"
      },
      "source": [
        "\n",
        "# 다른변수들이 'amount'와 같은 pattern을 가지는 지 확인¶\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3lFAllZj1pl"
      },
      "source": [
        "def plot_model_graph(data_frame, store_num, idx_col, y_cols):\n",
        "    df_set = data_frame[data_frame.store_id == store_num]\n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(15, 3))\n",
        "    ax1.set_xlabel(\"{}\".format(idx_col))\n",
        "    ax1.set_ylabel(\"{}\".format(y_cols[0])).set_color(color='tab:blue')\n",
        "    ax1.plot(df_set.index, df_set[y_cols[0]].values, color='tab:blue')\n",
        "    \n",
        "    for i in range(1, len(y_cols)):\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.set_ylabel(\"{}\".format(y_cols[i])).set_color(color='C{}'.format(i))\n",
        "        ax2.plot(df_set.index, df_set[y_cols[i]].values, color='C{}'.format(i))\n",
        "        ax2.spines['right'].set_position(('outward', (60*(i-1))))\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU7PmTsmkL8s"
      },
      "source": [
        "for i in df_month.store_id.unique()[:3]:\n",
        "    plot_model_graph(df_month, i, 'transacted_date', ['amount', 'real_tot_day', 'num_of_pay'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFAlOGDMkhSV"
      },
      "source": [
        "# # Step3. Modeling - Time Series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E55K8dEclJgP"
      },
      "source": [
        "Simple Moving Average\n",
        "\n",
        "단순이동평균은 특정 기간 동안의 data를 단순 평균하여 계산한다. 따라서 그 기간 동안의 data를 대표하는 값이 이동평균 안에는 그 동안의 data 움직임을 포함하고 있다.\n",
        "이동평균의 특징인 지연(lag)이 발생하며 수학적으로 n/2 시간 만큼의 지연이 발생한다.\n",
        "단순이동평균은 모든 데이터의 중요도를 동일하다고 간주한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuBuPQw0kiQi"
      },
      "source": [
        "def make_sma_arr(window_num):\n",
        "    ma_arr = np.array([])\n",
        "    for i in df_month.store_id.unique():\n",
        "        df_set = df_month[df_month.store_id == i]\n",
        "        ma_arr = np.concatenate((ma_arr, df_set.amount.rolling(window=window_num).mean().values))\n",
        "        \n",
        "    return ma_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJMMvsLRni7d"
      },
      "source": [
        "sma_month = df_month.copy()\n",
        "\n",
        "sma_month.insert(7, 'amount_2ma', make_sma_arr(2))\n",
        "sma_month.insert(8, 'amount_3ma', make_sma_arr(3))\n",
        "sma_month.insert(9, 'amount_6ma', make_sma_arr(6))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5W3yluTnkO0"
      },
      "source": [
        "sma_month.head(7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCb661lnlRr"
      },
      "source": [
        "for i in sma_month.store_id.unique()[:3]:\n",
        "    plot_model_graph(sma_month, i, 'transacted_date', ['amount', 'amount_2ma', 'amount_3ma', 'amount_6ma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDmNuhognuz6"
      },
      "source": [
        "print(\"2 winodw SMA MAE Score : \", mae(sma_month.amount_2ma.fillna(0), sma_month.amount.values))\n",
        "print(\"3 window SMA MAE Score : \", mae(sma_month.amount_3ma.fillna(0), sma_month.amount.values))\n",
        "print(\"6 window SMA MAE Score : \", mae(sma_month.amount_6ma.fillna(0), sma_month.amount.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuL0UQGkoj3T"
      },
      "source": [
        "def make_minus_rolling(data_frame, rolling_num):\n",
        "    def minus_shift_rolling(df_num, num):\n",
        "        a = np.average(df_num.values[-num:])\n",
        "        b = np.average(np.append(df_set.values[-(num-1):], a))\n",
        "        if num > 2:\n",
        "            c = np.average(np.append(np.append(df_set.values[-(num-2):], a), b))\n",
        "        else:\n",
        "            c = np.average((a, b))\n",
        "        return np.sum((a, b, c))\n",
        "\n",
        "    minus_rolling_arr = np.array([])\n",
        "    for i in data_frame.store_id.unique():\n",
        "        df_set = pd.DataFrame(data_frame[data_frame.store_id == i].amount)\n",
        "        minus_rolling_arr = np.concatenate((minus_rolling_arr, np.array([minus_shift_rolling(df_set, rolling_num)])))\n",
        "        \n",
        "    df_rolling = pd.DataFrame({'store_Id' : df_sub.store_id, 'amount' : minus_rolling_arr})\n",
        "    \n",
        "    return df_rolling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojs_GhIXonvp"
      },
      "source": [
        "for i in range(2, 5):\n",
        "    df_rolling = make_minus_rolling(df_month, i)\n",
        "    df_rolling.to_csv('/content/drive/My Drive/data/dacon/funda_{}_rolling_sub.csv'.format(i), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCsVxJAloo_h"
      },
      "source": [
        "def make_ewm_arr(data_frame, span_num):\n",
        "    arr_ewm = np.array([])\n",
        "    for i in data_frame.store_id.unique():\n",
        "        df_set = data_frame[data_frame.store_id == i]\n",
        "        # 여기에서 지정하는 span값은 위 수식에서 N에 해당한다.\n",
        "        arr_ewm = np.concatenate((arr_ewm, df_set.amount.ewm(span=span_num).mean().values))\n",
        "    \n",
        "    return arr_ewm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Kdrzu-pOJ_"
      },
      "source": [
        "ewm_month = df_month.copy()\n",
        "\n",
        "ewm_month.insert(7, 'amount_3ewm', make_ewm_arr(df_month, 3))\n",
        "ewm_month.insert(8, 'amount_6ewm', make_ewm_arr(df_month, 6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkRktLvbpQgc"
      },
      "source": [
        "ewm_month.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IndOyP11pRte"
      },
      "source": [
        "for i in ewm_month.store_id.unique()[:3]:\n",
        "    plot_model_graph(ewm_month, i, 'transacted_date', ['amount', 'amount_3ewm', 'amount_6ewm'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnrOuN1_pTPH"
      },
      "source": [
        "print(\"3 N EWM MAE Score : \", mae(ewm_month.amount_3ewm, sma_month.amount.values))\n",
        "print(\"6 N EWM MAE Score : \", mae(ewm_month.amount_6ewm, sma_month.amount.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yUGsFzLpaCS"
      },
      "source": [
        "def make_wma_sub(data_frame, span_num):\n",
        "    concat_3mon = pd.DataFrame(index=pd.to_datetime(['2019-03-31', '2019-04-30', '2019-05-31']))\n",
        "    wma_sub = np.array([])\n",
        "\n",
        "    for i in df_month.store_id.unique():\n",
        "        df_set = pd.DataFrame(data_frame[data_frame.store_id == i].amount)\n",
        "        wma_train = pd.concat([df_set, concat_3mon], axis=0)\n",
        "\n",
        "        num_sub = np.array([wma_train.amount.ewm(span=span_num).mean()['2019-03':].sum()])\n",
        "        print(num_sub)\n",
        "        wma_sub = np.concatenate((wma_sub, num_sub))\n",
        "        \n",
        "    df_wma_sub = pd.DataFrame({'store_id' : df_sub.store_id, 'amount' : wma_sub})\n",
        "    \n",
        "    return df_wma_sub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP5k1W88pt5k"
      },
      "source": [
        "for i in range(2, 7):\n",
        "    wma_sub = make_wma_sub(df_month, i)\n",
        "    break\n",
        "    wma_sub.to_csv('/content/drive/My Drive/data/dacon/funda_{}wma_sub.csv'.format(i), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rLSFByPqU2r"
      },
      "source": [
        "Exponential Smoothing (지수평활법 - Simple, Holt's, Holt-Winter's)\n",
        "1) Simple Exponential Smoothing\n",
        "trend나 seasonality 반영을 하지 못함\n",
        "level 정도만 수평선으로 나옴\n",
        "Ft=Ft−1+α(Dt−1−Ft−1)\n",
        " \n",
        "Ft=(1−α)Ft−1+αDt−1\n",
        " \n",
        "Ft  : 현재 시점의 예측 값\n",
        "Ft−1  : 이전 시점의 예측 값\n",
        "Dt−1  : 이전 시점의 실제 값\n",
        "α  : smoothing 요소, 0 <  α  < 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY2Ry2_EpvHB"
      },
      "source": [
        "df_set = df_month[df_month.store_id == 0]\n",
        "\n",
        "size = int(len(df_set) * 0.7)\n",
        "train = df_set[:size]\n",
        "test = df_set[size:]\n",
        "\n",
        "ses_model = SimpleExpSmoothing(train.amount)\n",
        "ses_result = ses_model.fit()\n",
        "ses_pred = ses_result.forecast(len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXT-TchyqstO"
      },
      "source": [
        "print(\"store_id 0 mean value : \", df_set.amount.mean())\n",
        "print(\"MAE Score of test :\", mae(test.amount, ses_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZVyLTENquDf"
      },
      "source": [
        "def plot_train_test_pred_graph(trainset, testset, pred):\n",
        "    plt.figure(figsize=(15,3))\n",
        "    plt.plot(trainset.amount, label='train')\n",
        "    plt.plot(testset.amount, label='test')\n",
        "    plt.plot(testset.index, pred, label='prediction')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwTV4ogeqvJP"
      },
      "source": [
        "plot_train_test_pred_graph(train, test, ses_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsGQ2bKCqwU_"
      },
      "source": [
        "ses_pred_arr = np.array([])\n",
        "for i in df_month.store_id.unique():\n",
        "    df_set = df_month[df_month.store_id == i]\n",
        "    \n",
        "    ses_model = SimpleExpSmoothing(df_set.amount)\n",
        "    ses_result = ses_model.fit()\n",
        "    ses_pred = ses_result.forecast(3)\n",
        "    \n",
        "    ses_pred_arr = np.concatenate((ses_pred_arr, np.array([ses_pred.sum()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBCsfbPqx0R"
      },
      "source": [
        "df_ses_sub = pd.DataFrame({'store_id' : df_sub.store_id, 'amount' : ses_pred_arr})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA9muCZeq-Ee"
      },
      "source": [
        "2) Holt's Exponential Smoothing\n",
        "trend로 데이터를 예측하기 위해 Simple Exponential Smoothing에서 확장한 것이다.\n",
        "예측을 위한 식 외에 level smoothing을 위한 식과 trend smoothing을 위한 식이 포함된다\n",
        "생성된 예측은 선형적으로 나타나기 때문에 예측 범위가 멀어질 수록 over-forecast 되는 경향이 있다.\n",
        "Forecast equation :\n",
        "y^t+h|t=lt+hbt\n",
        " \n",
        "Level equation :\n",
        "lt=αyt+(1−α)(lt−1+bt−1)\n",
        " \n",
        "Trend Equation :\n",
        "bt=β∗(lt−lt−1)+(1−β∗)bt−1\n",
        " \n",
        "lt  : t 시점에서의 level(수준)의 추정을 나타낸다.\n",
        "\n",
        "bt  : t 시점에서의 추세(경사)의 추정을 나타낸다.\n",
        "α  : level(수준)에 대한 smoothing parameter이고 다음과 같음 범위를 갖는다. 0 <  α  < 1\n",
        "β∗  : trend(추세)에 대한 smoothing parameter이고 다음과 같은 범위를 갖는다. 0 <  β∗  < 1\n",
        "level에 대한 식  lt 는 t 시간에 대한 관측치  yt 와 훈련 예측  lt−1+bt−1 의 가중 평균을 나타낸다.\n",
        "trend에 대한 식  bt 는  (lt−lt−1) 에 근거한 t시간에 대한 추정치와 이전 추정치인  bt−1 의 가중 평균을 나타낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIW2cuAmqzPH"
      },
      "source": [
        "df_set = df_month[df_month.store_id == 0]\n",
        "\n",
        "size = int(len(df_set) * 0.7)\n",
        "train = df_set[:size]\n",
        "test = df_set[size:]\n",
        "\n",
        "holt_model = Holt(np.array(train.amount))\n",
        "holt_result = holt_model.fit()\n",
        "holt_pred = holt_result.forecast(len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVEqyjXqtQ1F"
      },
      "source": [
        "print(\"Mean value of store_id 0 : \", df_set.amount.mean())\n",
        "print(\"MAE Score of test :\", mae(test.amount, holt_pred))\n",
        "\n",
        "plot_train_test_pred_graph(train, test, holt_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCmaBi1WtSCY"
      },
      "source": [
        "%%time\n",
        "holt_pred_arr = np.array([])\n",
        "for i in df_month.store_id.unique():\n",
        "    df_set = df_month[df_month.store_id == i]\n",
        "    \n",
        "    holt_model = Holt(np.array(df_set.amount))\n",
        "    holt_result = holt_model.fit()\n",
        "    holt_pred = holt_result.forecast(3)\n",
        "    \n",
        "    holt_pred_arr = np.concatenate((holt_pred_arr, np.array([holt_pred.sum()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmctXyiEtTuz"
      },
      "source": [
        "holt_sub = pd.DataFrame({'sotre_id' : df_sub.store_id, 'amount' : holt_pred_arr})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhrnf4_NtZeG"
      },
      "source": [
        "3) Holt-Winter's Exponential Smoothing\n",
        "Holt-Winter's 방법은 seasonality를 반영하기 위해 Holt's 방법에서 확장된 것이다.\n",
        "Holt-Winter's 방법은 예측식과 3개의 smoothing 식으로 구성되어 있다.\n",
        "level에 대한 식  lt \n",
        "trend에 대한 식  b−t \n",
        "seasonal에 대한 요소  st \n",
        "smoothing parameter에 해당하는  α ,  β∗ ,  γ \n",
        "seasonality의 빈도를 나타내기 위한  m \n",
        "seasonal이 변화하는 형태에 따라 두 가지 방법이 있다.\n",
        "additive : seasonal의 변화가 일정하게 지속될 때\n",
        "multiplicative : seasonal의 변화가 level에 비례적일 때"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-qJwGREtUvf"
      },
      "source": [
        "df_set = df_month[df_month.store_id == 0]\n",
        "\n",
        "size = int(len(df_set) * 0.7)\n",
        "train = df_set[:size]\n",
        "test = df_set[size:]\n",
        "\n",
        "# 최적값을 찾기 위한 기준\n",
        "es_score = 1000000000\n",
        "best_period = 0\n",
        "# 각 store_id 최적의 seasonal period를 찾기 위한 탐색\n",
        "for p in range(2,(len(train)//2) + 1):\n",
        "    try:\n",
        "        es_model = ExponentialSmoothing(np.array(train.amount), seasonal_periods=p, trend='add', seasonal='add')\n",
        "        es_result = es_model.fit()\n",
        "        es_pred = es_result.forecast(len(test))\n",
        "\n",
        "        if es_score > mae(test.amount, es_pred):\n",
        "            es_score = mae(test.amount, es_pred)\n",
        "            best_period = p\n",
        "    except:\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZS21oBtck9"
      },
      "source": [
        "print(\"Mean value of store_id 0 :\", df_set.amount.mean())\n",
        "print(\"MAE Score of test :\", es_score)\n",
        "print(\"Best seasonal period :\", best_period)\n",
        "\n",
        "plot_train_test_pred_graph(train, test, es_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9erKW_Ntdq4"
      },
      "source": [
        "%%time\n",
        "holt_winters_arr = np.array([])\n",
        "for i in df_month.store_id.unique():\n",
        "    df_set = df_month[df_month.store_id == i]\n",
        "\n",
        "    size = int(len(df_set) * 0.7)\n",
        "    train = df_set[:size]\n",
        "    test = df_set[size:]\n",
        "    \n",
        "    # 최적값을 찾기 위한 기준\n",
        "    es_score = 1000000000\n",
        "    best_period = 0\n",
        "    # 각 store_id 최적의 seasonal period를 찾기 위한 탐색\n",
        "    for p in range(2,(len(train)//2) + 1):\n",
        "        try:\n",
        "            es_model = ExponentialSmoothing(np.array(train.amount),seasonal_periods=p, trend='add', seasonal='add')\n",
        "            es_result = es_model.fit()\n",
        "            es_pred = es_result.forecast(len(test))\n",
        "\n",
        "            if es_score > mae(test.amount, es_pred):\n",
        "                es_score = mae(test.amount, es_pred)\n",
        "                best_period = p\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    # 최적의 parameter을 이용한 3개월 예측과 sum\n",
        "    set_es_model = ExponentialSmoothing(np.array(df_set.amount), seasonal_periods=best_period, trend='add', seasonal='add')\n",
        "    set_es_result = set_es_model.fit()\n",
        "    set_es_pred = set_es_result.forecast(3)\n",
        "    \n",
        "    holt_winters_arr = np.concatenate((holt_winters_arr, np.array([set_es_pred.sum()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xPw6hI7tfXt"
      },
      "source": [
        "holt_winters_sub = pd.DataFrame({'store_id' : df_sub.store_id, 'amount' : holt_winters_arr})\n",
        "holt_winters_sub.to_csv('/content/drive/My Drive/data/dacon/funda_holt_winters_sub.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymgf6fM9t-ok"
      },
      "source": [
        "##ARIMA(Autoregressive Integrated Moving Average)\n",
        "ARIMA 모델은  Yt 을 차분(difference)한 결과로 만들어지 시계열  ∇Yt=Yt−Yt−1 이 ARMA 모형을 따르면 원래의 시계열  Yt 를 ARIMA 모형이라고 한다.\n",
        "만약  d 번 차분한 후예야 시계열  ∇dYt 가 ARMA(p,q) 모형을 따른다면 적분 차수가(order of integration)가 d인 ARIMA 모형으로 ARIMA(p, d, q)로 표기한다.\n",
        "q=0인 경우에는 ARI(p,d), q=0인 경우에는 IMA(d,q)로 표기한다.\n",
        "p, d, q의 조합을 탐색하며 최적 parameter를 찾고 기준은 fit에 저장되어 있는 AIC(Akaike's Information Criterion)을 기준으로 한다. 다음과 같은 식을 가지며 작을 수록 좋은 모형이다.\n",
        "AIC=−2log(Likelihood)+2K\n",
        " \n",
        "k  : 모델의 추정된 parameter의 갯수\n",
        "Likelihood  : 모델의 likelihoood function의 최댓값"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnPEnqdZtuGZ"
      },
      "source": [
        "p = list(range(0, 6))\n",
        "d = [0, 1, 2]\n",
        "q = list(range(0, 6))\n",
        "\n",
        "pdq = list(itertools.product(p, d, q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8aiZPCuFz_"
      },
      "source": [
        "%%time\n",
        "df_set = df_month[df_month.store_id == 0]\n",
        "\n",
        "size = int(len(df_set) * 0.7)\n",
        "train = df_set[:size]\n",
        "test = df_set[size:]\n",
        "\n",
        "best_score = 10000000\n",
        "best_param = 0\n",
        "for param in pdq:\n",
        "    try:\n",
        "        arima_model = ARIMA(train.amount.values, order=param)\n",
        "        result = arima_model.fit()\n",
        "        if result.aic < best_score:\n",
        "            best_score = result.aic\n",
        "            best_param = param\n",
        "    except:\n",
        "        continue\n",
        "        \n",
        "set_arima = ARIMA(df_set.amount.values, order=best_param)\n",
        "set_result = set_arima.fit()\n",
        "set_pred = set_result.forecast(len(test))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9VcIoe6uJEy"
      },
      "source": [
        "print(\"Mean value of store_id 0 :\", df_set.amount.mean())\n",
        "print(\"AIC Score of test :\", best_score)\n",
        "print(\"Best parameter of (p, d, q): \",best_param)\n",
        "\n",
        "plot_train_test_pred_graph(train, test, set_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtbYWOg1uKJv"
      },
      "source": [
        "%%time\n",
        "arima_pred_arr = np.array([])\n",
        "for i in df_month.store_id.unique()[:10]:\n",
        "    df_set = df_month[df_month.store_id == i]\n",
        "\n",
        "    best_score = 10000000000\n",
        "    best_param = 0\n",
        "    for param in pdq:\n",
        "        try:\n",
        "            arima_model = ARIMA(df_set.amount.values, order=param)\n",
        "            result = arima_model.fit()\n",
        "            # 최적 parameter는 fit에 저장되어 있는 AIC값을 기준으로 선정 한다.\n",
        "            if result.aic < best_score:\n",
        "                best_score = result.aic\n",
        "                best_param = param\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    \n",
        "    arima_model = ARIMA(df_set.amount.values, order=best_param)\n",
        "    arima_result = arima_model.fit()\n",
        "    arima_pred = arima_result.forecast(3)[0]\n",
        "\n",
        "    arima_pred_arr = np.concatenate((arima_pred_arr, np.array([arima_pred.sum()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRhp_KEouLnL"
      },
      "source": [
        "arima_sum_sub = pd.DataFrame({'store_id' : df_sub.store_id, 'amount' : arima_pred_arr})\n",
        "arima_sum_sub.to_csv('/content/drive/My Drive/data/dacon/funda_arima_front_sum_sub.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}