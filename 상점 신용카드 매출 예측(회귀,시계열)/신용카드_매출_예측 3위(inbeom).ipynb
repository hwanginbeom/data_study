{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "신용카드 매출 예측(03.21).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxqLqBp2nNRr"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIu9dkF8oWBn"
      },
      "source": [
        "pip install shap==0.23.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHNr05UEoeNh"
      },
      "source": [
        "pip install workalendar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx2eh7thni4_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import shap\n",
        "\n",
        "import lightgbm as lgbm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from workalendar.asia import SouthKorea\n",
        "\n",
        "from sklearn.impute import SimpleImputer # 결측치 처리 \n",
        "from sklearn.preprocessing import LabelEncoder # 라벨 인코더\n",
        "from sklearn.decomposition import TruncatedSVD # 특이값 분해\n",
        "from sklearn.preprocessing import StandardScaler # 표준화\n",
        "from sklearn.preprocessing import PolynomialFeatures # 다항회귀"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VKDGa-BoM-T"
      },
      "source": [
        "# Loading data.\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/dacon/funda_train.csv')\n",
        "\n",
        "data['date'] = data['transacted_date'] + ' ' + data['transacted_time']\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDWA8iTopC_"
      },
      "source": [
        "data[data.amount < 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP5jvnkQSSUw"
      },
      "source": [
        "# 2. 데이터 전처리\n",
        "Data Cleansing & Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpEalpnkovoJ"
      },
      "source": [
        "# Removing negative values. # 음수값 제거하기\n",
        "\n",
        "def remove_negative(data):\n",
        "    data_pos = data[data.amount > 0]\n",
        "    data_neg = data[data.amount < 0]\n",
        "    drop_ind = set()\n",
        "    \n",
        "    for neg in data_neg.itertuples():\n",
        "        amount = abs(neg.amount) # 절대값\n",
        "        \n",
        "        row = data_pos[data_pos.store_id == neg.store_id] # 스토어 아이디\n",
        "        row = row[row.card_id == neg.card_id] # 카드아이디\n",
        "        row = row[row.card_company == neg.card_company] # 회사 같은지\n",
        "        row = row[row.amount >= amount] # \n",
        "        row = row[row.date <= neg.date] # 음수가 더 많은 것?\n",
        "        row = row[~row.index.isin(drop_ind)] # 중복제거하는거 같은데 \n",
        "        \n",
        "        if len(row[row.amount == amount]) > 0: # 값이 같은지\n",
        "            row = row[row.amount == amount]\n",
        "            matched_row = row[row.date == max(row.date)]\n",
        "            drop_ind.update(matched_row.index) # matched_row의 index를 가져와서 drop_ind를 설정하고\n",
        "        elif len(row[row.amount > 0]):\n",
        "            matched_row = row[row.date == max(row.date)]\n",
        "            data_pos.loc[matched_row.index, 'amount'] += neg.amount\n",
        "            \n",
        "    data_pos.drop(drop_ind, axis = 0, inplace = True) # 제거한다. \n",
        "    print('Count - with negative: {}'.format(len(data)))\n",
        "    print('Count - without negative: {}'.format(len(data_pos)))\n",
        "\n",
        "    return data_pos\n",
        "    \n",
        "data = remove_negative(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPp2S8o0owHQ"
      },
      "source": [
        "# Removing unnecessary columns. # 안쓰는 컬럼 지우고 \n",
        "\n",
        "drop_cols = ['installment_term', 'transacted_date', 'transacted_time']\n",
        "data.drop(drop_cols, axis = 1, inplace = True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n93c7vlJo-aG"
      },
      "source": [
        "# Business-related features expansion. (비즈니스 관련 기능 확장)\n",
        "\n",
        "# I grouped different bussiness in smaller categories and organized them into a separate .CSV ( 여러 비즈니스를 더 작은 범주로 그룹화하고 별도의 .CSV로 구성했습니다.)\n",
        "\n",
        "biz_info = pd.read_csv('../data/business_info.csv')\n",
        "data = data.merge(biz_info, on = 'type_of_business', how = 'left')\n",
        "\n",
        "# Numerating strings as categories.\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "\n",
        "# I grouped different bussiness in smaller categories and organized them into a separate .CSV\n",
        "\n",
        "biz_info = pd.read_csv('../data/business_info.csv')\n",
        "data = data.merge(biz_info, on = 'type_of_business', how = 'left')\n",
        "\n",
        "# Numerating strings as categories.\n",
        "\n",
        "encoder = LabelEncoder() # 라벨인코더 하고 \n",
        "text_cols = data.dtypes.pipe(lambda x: x[x == 'object']).index.tolist()\n",
        "data[text_cols] = data[text_cols].apply(lambda x: encoder.fit_transform(x.tolist()))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69RJi7C4pD6E"
      },
      "source": [
        "# Date-related features extraction and preprocessing. #날짜관련 기능 전처리 \n",
        "\n",
        "cal = SouthKorea()\n",
        "\n",
        "data['weekday'] = data['date'].dt.weekday # 주차\n",
        "data['holiday'] = data['date'].apply(lambda x: cal.is_holiday(x)).astype(int) # 휴일\n",
        "data['workday'] = data['date'].apply(lambda x: cal.is_working_day(x)).astype(int) # workday\n",
        "\n",
        "data.set_index('date', drop = False, inplace = True) # date를 인덱스로 사용\n",
        "data.sort_index(inplace = True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn2Lgxtgry5G"
      },
      "source": [
        "# Active stores. #활동하고 있는 매장 구하기 \n",
        "\n",
        "active_stores = data.last('3M')['store_id'].unique() # 스토어의 유니크 키를 가져오고\n",
        "data['is_active'] = data['store_id'].isin(active_stores).astype(int) #지금 활동하고 있는 스토어를 가져온다.\n",
        "\n",
        "# Global aggregates by store.\n",
        "\n",
        "transforms = ['sum', 'mean', 'median', 'size', 'std', 'min', 'max']\n",
        "group = data.groupby('store_id')\n",
        "\n",
        "for t in transforms:  # 각각 지점의 합 , 평균, 중앙, 크기 , 표준편차, 최소 , 최대값 구하기\n",
        "    data[t] = group['amount'].transform(t)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oTZaS-Qr0Wf"
      },
      "source": [
        "# One-hot encoding. # 더미화 하기 \n",
        "\n",
        "dummy_cols = ['card_company', 'weekday'] # 이 두개만 따로\n",
        "\n",
        "for col in dummy_cols:\n",
        "    data = pd.concat([data, pd.get_dummies(data[col], prefix = col)], axis=1)\n",
        "\n",
        "data.drop(dummy_cols, axis = 1, inplace = True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z83TV-BR2sm"
      },
      "source": [
        "# 3. 탐색적 자료분석\n",
        "Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFxwMs1Xr2wD"
      },
      "source": [
        "# Adding moving averages and last-N stats for 3 months groups.\n",
        "# 3 개월 그룹에 대한 이동 평균 및 마지막 N 통계 추가.\n",
        "\n",
        "data['group_id'] = data.groupby(pd.Grouper(level = 0, freq = '1QS-DEC')).ngroup() # 날짜를 월별로 그룹화 한 것을 변수로 저장\n",
        "\n",
        "days = [1, 7, 14, 28, 46]\n",
        "sid_list = data['store_id'].unique() # 유니크한 스토어 아이디\n",
        "gid_list = data['group_id'].unique() # 유니크한 그룹 아이디\n",
        "\n",
        "sales = data.groupby(['store_id', 'group_id', pd.Grouper(level = 0, freq = '1D')])[['amount']].sum() # 이 두가지 그룹한 것을 토대로 합계를 구해 판매액을 구한다.\n",
        "features = []\n",
        "\n",
        "for sid in sid_list:\n",
        "    for gid in gid_list:\n",
        "        try:\n",
        "            store = sales.xs(sid, level = 0).xs(gid, level = 0)\n",
        "        except Exception as error:\n",
        "            continue\n",
        "            \n",
        "        dictionary = {'store_id': sid, 'group_id': gid}\n",
        "        \n",
        "        # Calculating decay rates. 붕괴율 계산? 단위시간당 분해되는 회계수 \n",
        "        \n",
        "        for d in days:\n",
        "            amount = store['amount'].resample('{}D'.format(d)).sum(min_count = 1).dropna()\n",
        "            date_dif = amount.reset_index()['date'].diff(-1).dt.days.abs()\n",
        "            \n",
        "            amount_dif = (amount.shift(-1) / amount).reset_index(drop = True)\n",
        "            dictionary['decay{}_mean'.format(d)] = amount_dif.mean()\n",
        "            dictionary['decay{}_mean~time'.format(d)] = (amount_dif / date_dif).mean()\n",
        "            \n",
        "            amount_dif = np.log1p(amount_dif)\n",
        "            dictionary['decay{}_log_mean'.format(d)] = (amount_dif / date_dif).mean()\n",
        "            dictionary['decay{}_log_mean~time'.format(d)] = (amount_dif / date_dif).mean()\n",
        "            \n",
        "        for d in days: \n",
        "            store['ma{}'.format(d)] = store['amount'].rolling('{}D'.format(d)).mean()\n",
        "            \n",
        "        for d1 in days:\n",
        "            l = store.last('{}D'.format(d1))\n",
        "            \n",
        "            dictionary['last{}_sum'.format(d1)] = l.amount.sum()\n",
        "            dictionary['last{}_mean'.format(d1)] = l.amount.mean()\n",
        "            dictionary['last{}_median'.format(d1)] = l.amount.median()\n",
        "            \n",
        "            for d2 in days:\n",
        "                dictionary['ma{}_last{}_mean'.format(d2, d1)] = l['ma{}'.format(d2)].mean()\n",
        "                \n",
        "        features.append(dictionary)\n",
        "            \n",
        "features = pd.DataFrame(features)\n",
        "\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNShE1JWr4sj"
      },
      "source": [
        "# Appending new features. (새로운 기능)\n",
        "\n",
        "ndata = data.merge(features, on = ['group_id', 'store_id'], how = 'left')\n",
        "ndata.set_index('date', drop = False, inplace = True)\n",
        "ndata.sort_index(inplace = True)\n",
        "\n",
        "# Aggregating data by 3 months duration.(3개월 기간으로 데이터 집계)\n",
        "\n",
        "aggregations = {\n",
        "    'amount': {\n",
        "        'sales_current': 'sum',\n",
        "        'sales_mean': 'mean',\n",
        "        'sales_median': 'median',\n",
        "        'sales_count': 'count',\n",
        "        'sales_std': 'std',\n",
        "        'sales_mad': 'mad',\n",
        "        'sales_min': 'min',\n",
        "        'sales_max': 'max'\n",
        "    },\n",
        "    'holiday': {\n",
        "        'holidays_%': 'mean',\n",
        "        'holidays_count': 'sum'\n",
        "    },\n",
        "    'workday': {\n",
        "        'workdays_%': 'mean',\n",
        "        'workdays_count': 'sum'\n",
        "    },\n",
        "    'card_id': {\n",
        "        'customers_unique': 'nunique',\n",
        "    },\n",
        "    'date': {\n",
        "        'year_min': lambda x: min(x).year,\n",
        "        'year_max': lambda x: max(x).year,\n",
        "        'doty_min': lambda x: min(x).dayofyear,\n",
        "        'doty_max': lambda x: max(x).dayofyear,\n",
        "        'time_span': lambda x: (max(x) - min(x)).days + 1\n",
        "    }\n",
        "}\n",
        "\n",
        "dummy_cols_agg = {col: {col: 'mean'} for col in ndata.columns if col.startswith(tuple(dummy_cols))}\n",
        "aggregations.update(dummy_cols_agg)\n",
        "\n",
        "col_regex = '|'.join(transforms)\n",
        "first_cols = ['region', 'type_of_business', 'business_area', 'kind_of_service', 'is_active']\n",
        "first_cols.extend(ndata.columns[ndata.columns.str.contains(col_regex, regex = True)])\n",
        "first_cols_agg = {col: {col: 'first'} for col in first_cols}\n",
        "aggregations.update(first_cols_agg)\n",
        "\n",
        "ndata = ndata.groupby(['store_id', pd.Grouper(level = 0, freq = '1QS-DEC')])\n",
        "ndata = ndata.agg(aggregations)\n",
        "ndata.columns = ndata.columns.droplevel(0)\n",
        "\n",
        "# Sales and moving averages lags.\n",
        "\n",
        "gdata = ndata.groupby(level = 0)\n",
        "lag_cols = set(first_cols) - set(transforms) - set(text_cols)\n",
        "\n",
        "for i in range(1, 9):\n",
        "    ndata['sales_diff_{}'.format(i)] = gdata['sales_current'].diff(i)\n",
        "    ndata['sales_last_{}'.format(i)] = gdata['sales_current'].shift(i)\n",
        "    ndata['sales_pctd_{}'.format(i)] = gdata['sales_current'].pct_change(i)\n",
        "    \n",
        "    ndata['sales_diff_last{}'.format(i)] = gdata['sales_current'].diff().shift(i)\n",
        "    ndata['sales_pctd_last{}'.format(i)] = gdata['sales_current'].pct_change().shift(i)\n",
        "    \n",
        "    ndata['sales_last_{}_current_%'.format(i)] = ndata['sales_last_{}'.format(i)] / ndata['sales_current']\n",
        "    ndata['sales_current_last_{}_%'.format(i)] = ndata['sales_current'] / ndata['sales_last_{}'.format(i)]\n",
        "    ndata['sales_dcay_{}'.format(i)] = ndata['sales_current'] / ndata['sales_last_{}'.format(i)]\n",
        "    ndata['sales_dcay{}_log'.format(i)] = np.log1p(ndata['sales_dcay_{}'.format(i)])\n",
        "    \n",
        "    for col in lag_cols:\n",
        "        ndata['{}_lag_{}'.format(col, i)] = gdata[col].shift(i)\n",
        "    \n",
        "# Additional features and feature interactions.(추가기능 및 기능 상호 작용)\n",
        "\n",
        "ndata['sales_mean_%'] = ndata['sales_mean'] / ndata['mean']\n",
        "ndata['sales_range'] = ndata['sales_max'] - ndata['sales_min']\n",
        "ndata['customers_unique_%'] = ndata['customers_unique'] / ndata['sales_count']\n",
        "ndata['sales_diff_1_mean'] = ndata.groupby(level = 0)['sales_diff_1'].transform('mean')\n",
        "ndata['sales_pctd_1_mean'] = ndata.groupby(level = 0)['sales_pctd_1'].transform('mean')\n",
        "ndata['sales_current_last_1_%_mean'] = ndata.groupby(level = 0)['sales_current_last_1_%'].transform('mean')\n",
        "\n",
        "for t in transforms: \n",
        "    ndata['rank_{}'.format(t)] = ndata[t].rank(method = 'dense', ascending = False)   \n",
        "    \n",
        "for d in days:\n",
        "    ndata['last{}_sum_current_%'.format(d)] = ndata['last{}_sum'.format(d)] / ndata['sales_current']    \n",
        "    \n",
        "ndata['range'] = ndata['max'] - ndata['min']\n",
        "ndata['rank_range'] = ndata['range'].rank(method = 'dense', ascending = False)\n",
        "ndata['sales_current_rank_sum_%'] = ndata['sales_current'] / ndata['rank_sum']\n",
        "\n",
        "for i in range(2, 9):\n",
        "    ndata['sales_diff_{}_diff_1_mean_%'.format(i)] = ndata['sales_diff_{}'.format(i)] / ndata['sales_diff_1_mean']\n",
        "    ndata['sales_pctd_{}_pctd_1_mean_%'.format(i)] = ndata['sales_pctd_{}'.format(i)] / ndata['sales_pctd_1_mean']\n",
        "    ndata['sales_pctd_{}_diff_1_mean_%'.format(i)] = ndata['sales_pctd_{}'.format(i)] / ndata['sales_diff_1_mean']\n",
        "    ndata['sales_diff_{}_pctd_1_mean_%'.format(i)] = ndata['sales_diff_{}'.format(i)] / ndata['sales_pctd_1_mean']\n",
        "\n",
        "    \n",
        "# Post-processing.\n",
        "    \n",
        "ndata.drop(['sum', 'size'], axis = 1, inplace = True)\n",
        "ndata.reset_index(inplace = True)    \n",
        "ndata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtgSMtYkr8YM"
      },
      "source": [
        "# Adding truncated SVD features. svd 알고리즘 돌리기 \n",
        "\n",
        "imp = SimpleImputer(strategy = \"constant\", fill_value = 0) # \n",
        "tr_svd = TruncatedSVD(n_components = 20)\n",
        "ex_cols = ['date', 'store_id', 'sales_current', 'sales_current_last_1_%', 'is_active',\n",
        "           'region', 'type_of_business', 'business_area', 'kind_of_service']\n",
        "\n",
        "x = ndata.loc[:, ~ndata.columns.isin(ex_cols)]\n",
        "scaled_x = StandardScaler().fit_transform(x.values) # 표준화\n",
        "imputed_x = imp.fit_transform(scaled_x) # 빈 값 대체하기 \n",
        "truncated_x = tr_svd.fit_transform(imputed_x) # svd 돌리기\n",
        "\n",
        "scaled_x = pd.DataFrame(scaled_x, columns = x.columns) # scaled_x 에 대한 데이터 프레임 만들고\n",
        "truncated_x = pd.DataFrame(truncated_x) # truncated_x 에 대한 데이터 프레임만든다.\n",
        "\n",
        "tdata = pd.concat([ndata[ex_cols], scaled_x, truncated_x], axis = 1) # 그리고 합친다.\n",
        "tdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915Tuu2BW9sQ"
      },
      "source": [
        "#4. 변수 선택 및 모델 구축\n",
        "Feature Engineering & Initial Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXJJkg0ar_Bm"
      },
      "source": [
        "# Preparing training and validation datasets. (훈련 및 검증 데이터세트 구하기)\n",
        "\n",
        "def split_data(data, tr_multiplier = 1, is_expanding = False, verbose = True):\n",
        "    n = len(data.date.unique()) - (tr_multiplier + 1)\n",
        "    group = list(data.groupby('date')) # date로 그룹화  한 것을 리스트로 가져온다. \n",
        "    group = [g[-1] for g in group] # 그룹하 한것을 하나씩 저장한다.\n",
        "    splits = []\n",
        "    \n",
        "    for i in range(0, n):\n",
        "        ceil = i + tr_multiplier\n",
        "        floor = 0 if is_expanding else i\n",
        "\n",
        "        train = pd.concat(group[floor : ceil])\n",
        "        train = train.sort_values('date').groupby('store_id').tail(1)\n",
        "        valid = group[ceil: ceil + 1][0]\n",
        "        y_val = []\n",
        "        \n",
        "        if verbose:\n",
        "            print('Fold {}: TRAIN - {} to {}'.format(i, train.date.min(), valid.date.min()))\n",
        "\n",
        "        for j in range(0, 2):\n",
        "            y = group[ceil + j : ceil + j + 1][0][['store_id', 'sales_current_last_1_%']]\n",
        "            y['sales_current_last_1_%'] = np.log1p(np.log1p(y['sales_current_last_1_%']))\n",
        "            y.columns = ['store_id', 'y']\n",
        "            y_val.append(y)\n",
        "            \n",
        "        train = train.merge(y_val[0], on = 'store_id', how = 'inner')\n",
        "        valid = valid.merge(y_val[-1], on = 'store_id', how = 'inner')\n",
        "\n",
        "        train = train.drop('date', axis = 1).reset_index(drop = True)\n",
        "        valid = valid.drop('date', axis = 1).reset_index(drop = True)\n",
        "        \n",
        "        splits.append((train, valid))\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "splits = split_data(tdata, 1, True)\n",
        "splits[0][0].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf3P33-sApW"
      },
      "source": [
        "# Fitting model and cross validation. (피팅 모델 및 교차검증하기.)\n",
        "\n",
        "metric = 'l1'\n",
        "cat_features = ['region', 'type_of_business', 'business_area', 'kind_of_service']\n",
        "\n",
        "def train_lgbm(splits, metric = 'l1', categories = '',\n",
        "               max_depth = 6, num_leaves = 48, feature_fraction = 0.8,\n",
        "               num_iterations = 3000, early_stopping = 100):\n",
        "    models = []\n",
        "    params = {\n",
        "        'learning_rate': 0.01,\n",
        "        'boosting': 'gbdt',\n",
        "        'objective': 'regression_l1',\n",
        "        'num_leaves': num_leaves,\n",
        "        'max_bin': 255,\n",
        "        'max_depth': max_depth,\n",
        "        'metric': metric,\n",
        "        'num_iterations': num_iterations,\n",
        "        'early_stopping': early_stopping,\n",
        "        'cat_smooth': 10,\n",
        "        'feature_fraction': feature_fraction\n",
        "    }\n",
        "\n",
        "    for (t, v) in splits:\n",
        "        d_train = lgbm.Dataset(t.drop('y', axis = 1), label = t['y'])\n",
        "        d_valid = lgbm.Dataset(v.drop('y', axis = 1), label = v['y'], reference = d_train)\n",
        "\n",
        "        evals_result = {}\n",
        "        clf = lgbm.train(params, d_train, valid_sets = [d_train, d_valid], \n",
        "                         evals_result = evals_result, verbose_eval = 1500,\n",
        "                         categorical_feature = categories)\n",
        "\n",
        "        model = {\n",
        "            'score': list(clf.best_score.values()),\n",
        "            'evals': evals_result,\n",
        "            'model': clf,\n",
        "            'best': clf.best_iteration\n",
        "        }\n",
        "\n",
        "        models.append((model, t, v))\n",
        "        \n",
        "    return models\n",
        "\n",
        "                         \n",
        "models = train_lgbm(splits, metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrDfL4ddsCUF"
      },
      "source": [
        "scores = [m['score'] for (m, _, _) in models]\n",
        "scores = [{'train': score[0][metric], 'valid': score[-1][metric]} for score in scores]\n",
        "scores = pd.DataFrame(scores)\n",
        "scores.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7XwmCPRWGyD"
      },
      "source": [
        "# 5. 모델 학습 및 검증\n",
        "Model Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBntIUeYsE2N"
      },
      "source": [
        "for (m, _, _) in models:\n",
        "    lgbm.plot_importance(m['model'], importance_type = 'gain', max_num_features = 15, figsize = (15, 6))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmoHvOmsGF9"
      },
      "source": [
        "for (m, _, _) in models:\n",
        "    lgbm.plot_metric(m['evals'], metric = metric, figsize = (15, 6))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbdgmzbFsG7E"
      },
      "source": [
        "# Making and plotting predictions for stores, which are common across all folds. (# 매장에 대한 예측을 만들고 플로팅합니다. 이는 모든 접기에 공통입니다.)\n",
        "\n",
        "sid_list = set()\n",
        "\n",
        "for (_, t, v) in models:    \n",
        "    ids = set(t.store_id) & set(v.store_id)\n",
        "    sid_list = sid_list & ids if bool(sid_list) else ids\n",
        "    \n",
        "for (m, _, v) in models:    \n",
        "    d_plot = v[v['store_id'].isin(sid_list)]\n",
        "    d_plot['y_pred'] = m['model'].predict(d_plot, num_iteration = m['best'])\n",
        "    d_plot = np.expm1(np.expm1(d_plot.set_index('store_id')[['y', 'y_pred']]))\n",
        "    d_plot['error'] = d_plot['y'] - d_plot['y_pred']\n",
        "    d_plot = d_plot.groupby('store_id')['error'].mean()\n",
        "    d_plot.sort_values(inplace = True)\n",
        "\n",
        "    d_plot.iloc[::20].plot.bar(figsize = (15, 6), title = 'Prediction error')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGcrUQ1lsKek"
      },
      "source": [
        "# Exploring feature interactions with SHAP.\n",
        "\n",
        "def plot_feature_interaction(data, interaction_values): \n",
        "    img = np.abs(interaction_values).sum(0)\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        img[i,i] = 0\n",
        "\n",
        "    inds = np.argsort(-img.sum(0))[:30]\n",
        "    img = img[inds,:][:,inds]\n",
        "\n",
        "    plt.figure(figsize = (10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.yticks(range(img.shape[0]), data.columns[inds], rotation = 0, horizontalalignment = \"right\")\n",
        "    plt.xticks(range(img.shape[0]), data.columns[inds], rotation = 50, horizontalalignment = \"left\")\n",
        "    plt.gca().xaxis.tick_top()\n",
        "    plt.show()\n",
        "\n",
        "for (m, _, v) in models:\n",
        "    valid = v.drop('y', axis = 1)\n",
        "    explainer = shap.TreeExplainer(m['model'])\n",
        "    shap_interaction_values = explainer.shap_interaction_values(valid)\n",
        "    plot_feature_interaction(valid, shap_interaction_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL-7f3cDsLYs"
      },
      "source": [
        "# Adding error-based features.\n",
        "\n",
        "def add_error_features(data, models):\n",
        "    errors = pd.DataFrame() \n",
        "    \n",
        "    for (m, t, v) in models:    \n",
        "        valid = v[['store_id', 'y']]\n",
        "        train = t[['store_id', 'y']]\n",
        "\n",
        "        valid['y_pred'] = m['model'].predict(v, num_iteration = m['best'])\n",
        "        train['y_pred'] = m['model'].predict(t, num_iteration = m['best'])\n",
        "\n",
        "        train = np.expm1(np.expm1(train.set_index('store_id')[['y', 'y_pred']])).reset_index()\n",
        "        valid = np.expm1(np.expm1(valid.set_index('store_id')[['y', 'y_pred']])).reset_index()\n",
        "\n",
        "        train['error'] = train['y'] - train['y_pred']\n",
        "        valid['error'] = valid['y'] - valid['y_pred']\n",
        "\n",
        "        merged = train.merge(valid, on = 'store_id', how = 'outer')[['store_id', 'error_x', 'error_y']]\n",
        "        errors = pd.concat([errors, merged], axis = 0)\n",
        "\n",
        "    errors = errors.groupby('store_id')[['error_x', 'error_y']].mean().reset_index()\n",
        "    errors['error_x_abs'], errors['error_y_abs'] = errors['error_x'].abs(), errors['error_y'].abs()\n",
        "    errors['error_y_x_%'] = errors['error_y'] / errors['error_x']\n",
        "\n",
        "    errors['error_abs_sum'] = errors['error_x_abs'] + errors['error_y_abs']\n",
        "    errors['error_abs_mean'] = 0.5 * errors['error_abs_sum'] \n",
        "    errors['error_sum'] = errors['error_x'] + errors['error_y']\n",
        "    errors['error_mean'] = 0.5 * errors['error_sum']\n",
        "\n",
        "    quantiles = [0, .01, .05, .2, .5, .8, .95, .99, 1]\n",
        "\n",
        "    error_cols = [col for col in errors.columns if col.startswith('error')]\n",
        "    for col in error_cols:\n",
        "        errors['{}_group'.format(col)] = pd.qcut(errors[col], quantiles, labels = False)\n",
        "        errors['rank_{}'.format(col)] = errors[col].rank(method = 'dense')\n",
        "\n",
        "    erdata = data.merge(errors, on = 'store_id', how = 'left')\n",
        "    \n",
        "    return erdata\n",
        "\n",
        "    \n",
        "erdata = add_error_features(tdata, models)\n",
        "erdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zNIdJcRsLa9"
      },
      "source": [
        "# We retrain the model with error-based features.\n",
        "\n",
        "ersplits = split_data(erdata, 1, True, False)\n",
        "ermodels = train_lgbm(ersplits, metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E1KG2c4sLdR"
      },
      "source": [
        "scores = [m['score'] for (m, _, _) in ermodels]\n",
        "scores = [{'train': score[0][metric], 'valid': score[-1][metric]} for score in scores]\n",
        "scores = pd.DataFrame(scores)\n",
        "scores.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZTj95t3sLfc"
      },
      "source": [
        "# We remove all of the features, which are not giving enough gains.\n",
        "\n",
        "important_features = set()\n",
        "\n",
        "for (m, _, v) in ermodels:\n",
        "    valid = v.drop('y', axis = 1)\n",
        "    explainer = shap.TreeExplainer(m['model'])\n",
        "    shap_values = explainer.shap_values(valid)\n",
        "    shap_sum = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "    importance_df = pd.DataFrame([valid.columns.tolist(), shap_sum.tolist()]).T\n",
        "    importance_df.columns = ['feature', 'shap_importance']\n",
        "    important_features |= set(importance_df[importance_df['shap_importance'] > 10 ** -3]['feature'])\n",
        "      \n",
        "len(important_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPAy7pGsLhv"
      },
      "source": [
        "idata = erdata[important_features | set(['date', 'store_id', 'sales_current', 'sales_current_last_1_%', 'is_active'])]\n",
        "isplits = split_data(idata, 1, True, False)\n",
        "imodels = train_lgbm(isplits, metric, max_depth = 5, num_leaves = 32, \n",
        "                     feature_fraction = 0.9, num_iterations = 2000, early_stopping = 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ULRlYk1sLkd"
      },
      "source": [
        "# We reorganize features by adding features interactions for most imporant features and then\n",
        "# (# 우리는 가장 중요한 기능에 대한 기능 상호 작용을 추가하여 기능을 재구성 한 다음)\n",
        "# factorizing the matrix by tr-svd.\n",
        "# tr-svd로 행렬 분해.\n",
        "\n",
        "\n",
        "poly = PolynomialFeatures(2, include_bias = False, interaction_only = True)\n",
        "\n",
        "z = idata.loc[:, ~idata.columns.isin(ex_cols)]\n",
        "imputed_z = imp.fit_transform(z.values)\n",
        "poly_z = poly.fit_transform(imputed_z)\n",
        "truncated_z = tr_svd.fit_transform(poly_z)\n",
        "truncated_z = pd.DataFrame(truncated_x)\n",
        "\n",
        "zdata = pd.concat([idata, truncated_z], axis = 1)\n",
        "zdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUOY_DF6sLmH"
      },
      "source": [
        "zsplits = split_data(zdata, 1, True, False)\n",
        "zmodels = train_lgbm(zsplits, metric, max_depth = 5, num_leaves = 32, \n",
        "                     feature_fraction = 0.9, num_iterations = 2000, early_stopping = 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZByq3-jWhM3"
      },
      "source": [
        "# 6. 결과 및 결언\n",
        "Conclusion & Discussion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ2dgzm-sZUO"
      },
      "source": [
        "# Getting submission data.\n",
        "\n",
        "def get_submission(sbm_data, models, average_out = False):    \n",
        "    test = sbm_data.sort_values('date').groupby('store_id').tail(1).drop('date', axis = 1)\n",
        "    test.sort_values('store_id', inplace = True)\n",
        "\n",
        "    m = models[-1][0]\n",
        "    prediction = m['model'].predict(test, num_iteration = m['best'])\n",
        "    prediction = np.expm1(np.expm1(prediction))\n",
        "\n",
        "    submission = test[['store_id', 'sales_current', 'is_active']]\n",
        "    submission['amount'] = submission['sales_current'] * prediction\n",
        "    submission.set_index('store_id', inplace = True)\n",
        "    print(submission['amount'].mean())\n",
        "    \n",
        "    if average_out:\n",
        "        submission['amount'] = 0.5 * submission['amount'] + 0.5 * submission['sales_current']\n",
        "        print(submission['amount'].mean())\n",
        "        \n",
        "    submission.loc[submission['is_active'] == 0, 'amount'] = 0\n",
        "    submission = submission[['amount']]\n",
        "    print(submission['amount'].mean())\n",
        "    \n",
        "    return submission\n",
        "    \n",
        "\n",
        "submission = get_submission(zdata, zmodels, True)    \n",
        "# submission.to_csv('../notes/submissions/submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9YR4CiGscvR"
      },
      "source": [
        "# Final thoughts:\n",
        "#\n",
        "# This model due to the nature of preprocessing suffers from leaks, but I didn't have\n",
        "# enough time to get through reengineering process.\n",
        "# It would have probably been a good idea to keep information about installments\n",
        "# and use it as an estimation for level of luxury for different shops.\n",
        "# I didn't work with N/A values in my solution and just left them as a separate category,\n",
        "# but playing around with using KNN to fill them might have given better results.\n",
        "# The worst part about this model is that it uses post-processing by averaging out results\n",
        "# of the last 3 months. \n",
        "# Another way to improve the model would have been to train many Lasso-regression models\n",
        "# separately for each store and use their predictions as features for LGBM model.\n",
        "# Working with lower granularity of data might have worked as well.\n",
        "#\n",
        "# Thank you!\n",
        "\n",
        "\n",
        "# 마지막 생각들:\n",
        "#\n",
        "# 이 모델은 전처리의 특성상 누수가 있지만\n",
        "# 리엔지니어링 과정을 거치기에 충분한 시간.\n",
        "# 할부 정보를 보관하는 것이 좋은 생각이었을 것입니다.\n",
        "# 다른 상점의 명품 수준에 대한 추정치로 사용합니다.\n",
        "# 내 솔루션에서 N / A 값으로 작업하지 않고 별도의 범주로 남겨 두었습니다.\n",
        "# 그러나 KNN을 사용하여이를 채우는 것이 더 나은 결과를 제공했을 수 있습니다.\n",
        "# 이 모델의 가장 나쁜 점은 결과를 평균화하여 후 처리를 사용한다는 것입니다.\n",
        "# 지난 3 개월 중 \n",
        "# 모델을 개선하는 또 다른 방법은 많은 Lasso-regression 모델을 훈련시키는 것입니다.\n",
        "# 각 상점에 대해 개별적으로 예측을 LGBM 모델의 기능으로 사용하십시오.\n",
        "# 낮은 단위의 데이터로 작업하는 것도 효과가있을 수 있습니다.\n",
        "#\n",
        "# 감사합니다!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brRL9RjFr5Bu"
      },
      "source": [
        "데이터 전처리\n",
        "1.카테고리 분류 다시함\n",
        "2. 음수를 제거함 \n",
        "3.안쓰는 컬럼제거\n",
        "4.휴일,일하는일, weekday\n",
        "5.실제로 운영하고 있는지에 대한 변수를 생성\n",
        "6.store_id그룹 - 기초 통계\n",
        "\n",
        "탐색적 자료분석\n",
        "1. 스토어 아이디 + 그룹아이디(아마도 월별 ) 묶어서 그룹화\n",
        "\n",
        "\n",
        "회귀를 했을떄 독립변수들이 여러개 있을때 \n",
        "\n",
        "최적의 모델을 변수를 선택하는데 back 은 제일 적은거다. \n",
        "\n",
        "변수선택법을 \n",
        "\n",
        "ㅇㅋㅇㅋ \n",
        "\n",
        "\n",
        "\n",
        "데이터 전처리\n",
        "1.카테고리 분류 다시함\n",
        "2. 음수를 제거함 \n",
        "3.안쓰는 컬럼제거\n",
        "4.휴일,일하는일, weekday\n",
        "5.실제로 운영하고 있는지에 대한 변수를 생성\n",
        "6.store_id그룹 - 기초 통계\n",
        "\n",
        "탐색적 자료분석\n",
        "1. 스토어 아이디 + 그룹아이디(아마도 월별 ) 묶어서 그룹화\n",
        "\n",
        "\n",
        "회귀를 했을떄 독립변수들이 여러개 있을때 \n",
        "\n",
        "최적의 모델을 변수를 선택하는데 back 은 제일 적은거다. \n",
        "\n",
        "변수선택법 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PolynomialFeatures\n",
        "\n",
        "다항 회귀는 비선형 데이터를 학습하기 위한 선형 모델인데\n",
        "변수를 뭐 거듭제곱하던지 해서 변수를 확장해서 훈련하는거래욥\n",
        "그래서 우리 데이터가 비선형 시계열 자료라서 다항회귀한듯\n",
        "\n",
        "\n",
        "비선형 효과를 회귀분석에 담기 위해 회귀모형을 확장하는 몇 가지 방법이 있다.\n",
        "다항식 회귀\n",
        "스플라인 회귀\n",
        "일반화가법모델(GAM)\n",
        "시계열데이터인 경우 AR, MA, ARIMA 등\n",
        "머신러닝 모델\n",
        "\n",
        "좋았다 집단지성 굳 \n",
        "\n",
        "PolynomialFeatures\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfRbxeSBr4HA"
      },
      "source": [
        ""
      ]
    }
  ]
}